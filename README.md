<!-- QuickGrad README -->

<h1 align="center">⚡ QuickGrad — Autograd Engine from Scratch</h1>
<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-blue?logo=python" />
  <img src="https://img.shields.io/badge/Autograd-Engine-orange?logo=codewars" />
  <img src="https://img.shields.io/badge/Inspiration-PyTorch-red?logo=pytorch" />
  <img src="https://img.shields.io/badge/Made%20for-Learning-green?logo=bookstack" />
</p>

<p align="center">
  🔬 A minimal, readable, NumPy-based automatic differentiation engine, built from scratch to understand how PyTorch works under the hood.
</p>

---

## 🌟 Features

- ✅ **Automatic Backpropagation** — Reverse-mode autodiff like PyTorch's `.backward()`
- 🧮 **Basic Tensor Class** — Supports scalar & multi-dimensional tensors with broadcasting
- ⚙️ **Computational Graph** — Builds dynamic graphs during forward pass
- 🎓 **Readable Code** — Designed for learning, debugging, and extending
- 📘 **Documentation & Demos** — Walkthrough notebooks to visualize how it works

---

## 🔍 Why QuickGrad?

> 🤯 Ever wondered how `x.backward()` works in PyTorch?

**QuickGrad** reveals the black box!  
Build, visualize, and debug your own computation graphs — while learning core deep learning mechanics.

---

## 📦 Installation

```bash
git clone https://github.com/navneetkumaryadav207001/quickgrad
cd quickgrad
pip install -r requirements.txt
