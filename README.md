<!-- QuickGrad README -->

<h1 align="center">âš¡ QuickGrad â€” Autograd Engine from Scratch</h1>
<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-blue?logo=python" />
  <img src="https://img.shields.io/badge/Autograd-Engine-orange?logo=codewars" />
  <img src="https://img.shields.io/badge/Inspiration-PyTorch-red?logo=pytorch" />
  <img src="https://img.shields.io/badge/Made%20for-Learning-green?logo=bookstack" />
</p>

<p align="center">
  ðŸ”¬ A minimal, readable, NumPy-based automatic differentiation engine, built from scratch to understand how PyTorch works under the hood.
</p>

---

## ðŸŒŸ Features

- âœ… **Automatic Backpropagation** â€” Reverse-mode autodiff like PyTorch's `.backward()`
- ðŸ§® **Basic Tensor Class** â€” Supports scalar & multi-dimensional tensors with broadcasting
- âš™ï¸ **Computational Graph** â€” Builds dynamic graphs during forward pass
- ðŸŽ“ **Readable Code** â€” Designed for learning, debugging, and extending
- ðŸ“˜ **Documentation & Demos** â€” Walkthrough notebooks to visualize how it works

---

## ðŸ” Why QuickGrad?

> ðŸ¤¯ Ever wondered how `x.backward()` works in PyTorch?

**QuickGrad** reveals the black box!  
Build, visualize, and debug your own computation graphs â€” while learning core deep learning mechanics.

---

## ðŸ“¦ Installation

```bash
git clone https://github.com/navneetkumaryadav207001/quickgrad
cd quickgrad
pip install -r requirements.txt
